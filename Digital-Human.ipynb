{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Gold - Debates in DH.txt\")  as fin:\n",
    "    debates_text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debates_nopunc = \"\".join([char for char in debates_text if char not in string.punctuation])\n",
    "debates_tokens = debates_nopunc.lower().split()\n",
    "debates_nostops = [token for token in debates_tokens if token not in ENGLISH_STOP_WORDS and len(token)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Flyover Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter(debates_nostops)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debates_bigrams = [debates_nostops[i]+'_'+debates_nostops[i+1] for i in range(len(debates_nostops)-1)]\n",
    "d = Counter(debates_bigrams)\n",
    "d.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n = 300\n",
    "window = 3\n",
    "model = 'exclude_ny' # options: 'all_tokens', 'exclude_ny', 'exclude_dh', 'exclude_ny_dh'\n",
    "\n",
    "top_words = [x for x,y in c.most_common(top_n)]\n",
    "top_set = set(top_words)\n",
    "top_dict = {word:[] for word in top_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword-Context Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(debates_nostops)):\n",
    "    \n",
    "    if model not in ['all_tokens', 'exclude_ny', 'exclude_dh', 'exclude_ny_dh']:\n",
    "        \n",
    "        print('NOT A VALID MODEL')\n",
    "        break\n",
    "        \n",
    "    if debates_nostops[i] in top_set:\n",
    "        for j in range(i-window,i+window+1):\n",
    "            if j!=i and (i-j)>0 and j<len(debates_nostops) and debates_nostops[j] in top_set:\n",
    "                if model == 'all_tokens':\n",
    "                    \n",
    "                    top_dict[debates_nostops[i]].append( debates_nostops[j] )\n",
    "                    \n",
    "                elif model == 'exclude_ny':\n",
    "                    \n",
    "                    if not (debates_nostops[j]=='york' and debates_nostops[j-1]=='new' ) and not (debates_nostops[j]=='new' and debates_nostops[j+1]=='york' ) :\n",
    "                        top_dict[debates_nostops[i]].append( debates_nostops[j] )\n",
    "                \n",
    "                elif model == 'exclude_dh':\n",
    "\n",
    "                    if not (debates_nostops[j][:5]=='human' and debates_nostops[j-1]=='digital' )\\\n",
    "                    and not (debates_nostops[j]=='digital' and debates_nostops[j+1][:5]=='human' ):\n",
    "                        top_dict[debates_nostops[i]].append( debates_nostops[j] )\n",
    "                        \n",
    "                elif model == 'exclude_ny_dh':\n",
    "\n",
    "                    if not (debates_nostops[j][:5]=='human' and debates_nostops[j-1]=='digital' )\\\n",
    "                    and not (debates_nostops[j]=='digital' and debates_nostops[j+1][:5]=='human' )\\\n",
    "                    and not (debates_nostops[j]=='york' and debates_nostops[j-1]=='new' )\\\n",
    "                    and not (debates_nostops[j]=='new' and debates_nostops[j+1]=='york' ) :\n",
    "                        top_dict[debates_nostops[i]].append( debates_nostops[j] )\n",
    "                \n",
    "for key in top_dict.keys():\n",
    "    top_dict[key] = dict(Counter(top_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM, Normalize, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DTM\n",
    "dv = DictVectorizer()\n",
    "dtm = dv.fit_transform( [top_dict[key] for key in top_dict.keys()] ).toarray()\n",
    "feat_list = dv.get_feature_names()\n",
    "\n",
    "# Laplace Smooth\n",
    "row_sums = dtm.sum(axis=1)\n",
    "dtm = (dtm + 1) / (row_sums[:, np.newaxis] + len(dtm))\n",
    "\n",
    "# PCA\n",
    "\n",
    "# Since sklearn's implementation of PCA is probabalistic, have found that\n",
    "# learning more components than plan to use results in greater stability\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(dtm)\n",
    "\n",
    "# Print variance explained by first two PCs\n",
    "pca.explained_variance_ratio_[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Smoothed DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. for viz in R\n",
    "pd.DataFrame(dtm, columns=feat_list, index=top_dict.keys()).to_csv('dtm_smooth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code adapted from: https://github.com/teddyroland/python-biplot\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "xvector = pca.components_[0] # see 'prcomp(my_data)$rotation' in R\n",
    "yvector = pca.components_[1]\n",
    "\n",
    "xs = pca.transform(dtm)[:,0] # see 'prcomp(my_data)$x' in R\n",
    "ys = pca.transform(dtm)[:,1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(len(xs)):\n",
    "# circles project documents (ie rows from dtm) as points onto PC axes\n",
    "    plt.plot(xs[i], ys[i], 'bo')\n",
    "    plt.text(xs[i]*1.02, ys[i]*1.02, list(top_dict.keys())[i], color='b')\n",
    "\n",
    "for i in range(len(xvector)):\n",
    "# arrows project features (ie columns from dtm) as vectors onto PC axes\n",
    "    plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n",
    "              color='r', width=0.000005, head_width=0.000025)\n",
    "    plt.text(xvector[i]*max(xs)*1.02, yvector[i]*max(ys)*1.02,\n",
    "            feat_list[i], color='r')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Magnitude, Dot Product with 'digital'/'humanities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading_df = pd.DataFrame(pca.components_[:2].T, columns = ['LOADING_1','LOADING_2'], index=feat_list)\n",
    "loading_df['MAGNITUDE'] = abs(loading_df['LOADING_1']) + abs(loading_df['LOADING_2'])\n",
    "\n",
    "pc_norm_l2 = pca.components_[:2].T/np.linalg.norm(pca.components_[:2].T, axis=1)[:, np.newaxis]\n",
    "\n",
    "d_dex = loading_df.index.tolist().index('digital')\n",
    "loading_df['DOT_DIGITAL'] = np.dot(pc_norm_l2[d_dex], pc_norm_l2.T)\n",
    "\n",
    "h_dex = loading_df.index.tolist().index('humanities')\n",
    "loading_df['DOT_HUMANITIES'] = np.dot(pc_norm_l2[h_dex], pc_norm_l2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loading_df.to_csv(model+'_loadings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
